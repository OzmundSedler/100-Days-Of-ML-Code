# 100-Days-Of-ML-Code
Hello world!
100 Days of Machine Learning Coding as proposed by Siraj Raval

First step — 2 big learning courses. I selected classic Stanford ML and Yandex ML specialization. I think I will translate collab notebooks with exercises in English if anybody will have an interest in them.

My related knowledge stack — python, a basic level of SQL, basics of math analysis linear algebra, probability theory. All the stuff you would expect to learn in a tech university. I am halfway through the Andrew NG ML course and the second Yandex course in the ML and Data analytics specialization.

#100DaysOfMLCode

## Week 1 - 21 hours :

**Theory** : 

Linear model. MSE. Regression. Regularization (l1,l2). Logistic regression. Fisher's linear discriminant. Generalized linear models. Prediction of probabilities and estimation of parameters.

Practical advices on linear models. Feature scaling. Straightening spaces. Work with categorical features. Work with Unbalanced data. Multi-class classification.

Naive Bayes. Links: [1](https://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/),
[2](http://dataaspirant.com/2017/02/20/gaussian-naive-bayes-classifier-implementation-python/)

Tensorflow piplines e.t.c. from io19. 

[Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) - Really great brief on the theme.

[The numpy chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb) - all written in notebooks, you can test everything, when reading. 


Decision tree. Overfitting. Stopping criteria. Gini Index and Information Gain. 

[Half of the pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

Tree pruning. Random Forest. Ensemble methods: bagging, boosting and stacking.OOB.

**Practice** : 

[Naive 1 notebook](https://colab.research.google.com/drive/1UnSuv1SsrW_8-uvIjCOKoE9UGhE3gdFT),
[Naive 2 notebook](https://colab.research.google.com/drive/1DdPtNxKLTjgk1PE1rp9xTb4IHSgyp6rF)

[Sklearn grid search](https://colab.research.google.com/drive/1TugvgI8zGuGqFPi8ckWCR4AtJSIfB6Ab)

[Bike sharing demand. Sklearn case 1](https://colab.research.google.com/drive/1z1P9LKDxNjkcKSrdHeK5Q0c_h1lI8opv)

[Bike sharing demand. Sklearn case 2](https://colab.research.google.com/drive/1fA_ySZ75olMrHuV-gt2G4nsWPQi9vLcD)

[Data preprocessing and logistic regression for binary classification tasks](https://colab.research.google.com/drive/15gX4xDvG_8BkqSts3EMbkz4W5ZivndL-)

[Decision trees](https://colab.research.google.com/drive/1t35u2Vf88RpG-UVKW368FLQlkSPZN_DP)

[Random forest](https://colab.research.google.com/drive/1FcLkZPbIF-do9TrS-nOudwETaR-NS9ze)

**Other** :

PyData Ann Arbor: Caitlin Hudon - [Building Data Science](https://www.youtube.com/watch?v=WlBamzdiwic)

Digging into collab and this github repo to make it beautiful :)

[Getting Started with TensorFlow 2.0 (Google I/O'19)](https://www.youtube.com/watch?v=lEljKc9ZtU8) #io19

[TensorFlow Extended: Machine Learning Pipelines and Model Understanding (Google I/O'19)](https://www.youtube.com/watch?v=drYM04t57tU)

[Google Colab Free GPU Tutorial – Deep Learning Turkey – Medium](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) - Treasure article

[Use Kaggle to start (and guide) your ML/ Data Science journey — Why and How](https://towardsdatascience.com/use-kaggle-to-start-and-guide-your-ml-data-science-journey-f09154baba35) 

[Make your Pandas apply functions faster using Parallel Processing](https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1)

## Week 2 - 15 hours :

**Theory** : 

More on gradient boosting. SGB. GB for classification and regression. GB for decision trees.

Neural networks. Tutorials from course below. One layer NN. Universal approximation theorem. Various activation functions. Multilayer NN. Optimization of NN parameters. GD, SGD.  Forward Propagation and Backpropagtion. Regularization.

**Practice** : 

First practice from week 4 - decision trees, bagging and random forest.

[Bagging and random forest](https://colab.research.google.com/drive/1UA6fSB1_edpchK3Y5yYv19KGtm6icw2y)

[Let's take a look at XGBoost](https://colab.research.google.com/drive/1k6vYhFmk8Zh029RcpcXv_6NKM3DaYiTP)

[Finished week 4. Gradient boosting practice.](https://colab.research.google.com/drive/1qVHT21XzXJ00iMi7ZCjocQcTNZkmss64)

3 tensorflow videos - [TensorFlow high-level APIs: Part 1 - loading data - YouTube](https://www.youtube.com/watch?v=oFFbKogYdfc)

https://www.coursera.org/learn/introduction-tensorflow/home/welcome - whole course

Finished [Pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

Checked notebook about decesion trees and random forests from there (it was pretty simple):

https://colab.research.google.com/drive/1HindO6SmgirUQOT3TOWj-hwJroUQ52Es

Made a notebook for this article:
[Introduction to Python Ensembles – Dataquest](https://www.dataquest.io/blog/introduction-to-ensembles/):

https://colab.research.google.com/drive/1OvgNn6dEQ7BSeC8NvKy4rFMtfTa1oLVU

**Other** :

[Wonderful article about ensemble methods](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)

## Week 3 (short one to start next one on Monday ) - 8 hours

**Theory** : 

Naive Bayes. Bayes' classifier with Maximum Likelihood Estimation. Bayes minimum risk decision rule. 

**Practice** : 

[Neural networks practice](https://colab.research.google.com/drive/1sjx1eiiVriHvpaS7trWov9EacTLDKYaz)

[Naive bayes options](https://colab.research.google.com/drive/1hyriE27Q7Mkb5-VpfA_I4jiQon31ltIf)

**Other** : 


