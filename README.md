# 100-Days-Of-ML-Code
Hello world!
100 Days of Machine Learning Coding as proposed by Siraj Raval

First step — 2 big learning courses. I selected classic Stanford ML and Yandex ML specialization. I think I will translate collab notebooks with exercises in English if anybody will have an interest in them.

My related knowledge stack — python, a basic level of SQL, basics of math analysis linear algebra, probability theory. All the stuff you would expect to learn in a tech university. I am halfway through the Andrew NG ML course and the second Yandex course in the ML and Data analytics specialization.

#100DaysOfMLCode

## Week 1 - 21 hours :

**Theory** : 

Linear model. MSE. Regression. Regularization (l1,l2). Logistic regression. Fisher's linear discriminant. Generalized linear models. Prediction of probabilities and estimation of parameters.

Practical advices on linear models. Feature scaling. Straightening spaces. Work with categorical features. Work with Unbalanced data. Multi-class classification.

Naive Bayes. Links: [1](https://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/),
[2](http://dataaspirant.com/2017/02/20/gaussian-naive-bayes-classifier-implementation-python/)

Tensorflow piplines e.t.c. from io19. 

[Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) - Really great brief on the theme.

[The numpy chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb) - all written in notebooks, you can test everything, when reading. 


Decision tree. Overfitting. Stopping criteria. Gini Index and Information Gain. 

[Half of the pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

Tree pruning. Random Forest. Ensemble methods: bagging, boosting and stacking.OOB.

**Practice** : 

[Naive 1 notebook](https://colab.research.google.com/drive/1UnSuv1SsrW_8-uvIjCOKoE9UGhE3gdFT),
[Naive 2 notebook](https://colab.research.google.com/drive/1DdPtNxKLTjgk1PE1rp9xTb4IHSgyp6rF)

[Sklearn grid search](https://colab.research.google.com/drive/1TugvgI8zGuGqFPi8ckWCR4AtJSIfB6Ab)

[Bike sharing demand. Sklearn case 1](https://colab.research.google.com/drive/1z1P9LKDxNjkcKSrdHeK5Q0c_h1lI8opv)

[Bike sharing demand. Sklearn case 2](https://colab.research.google.com/drive/1fA_ySZ75olMrHuV-gt2G4nsWPQi9vLcD)

[Data preprocessing and logistic regression for binary classification tasks](https://colab.research.google.com/drive/15gX4xDvG_8BkqSts3EMbkz4W5ZivndL-)

[Decision trees](https://colab.research.google.com/drive/1t35u2Vf88RpG-UVKW368FLQlkSPZN_DP)

[Random forest](https://colab.research.google.com/drive/1FcLkZPbIF-do9TrS-nOudwETaR-NS9ze)

**Other** :

PyData Ann Arbor: Caitlin Hudon - [Building Data Science](https://www.youtube.com/watch?v=WlBamzdiwic)

Digging into collab and this github repo to make it beautiful :)

[Getting Started with TensorFlow 2.0 (Google I/O'19)](https://www.youtube.com/watch?v=lEljKc9ZtU8) #io19

[TensorFlow Extended: Machine Learning Pipelines and Model Understanding (Google I/O'19)](https://www.youtube.com/watch?v=drYM04t57tU)

[Google Colab Free GPU Tutorial – Deep Learning Turkey – Medium](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) - Treasure article

[Use Kaggle to start (and guide) your ML/ Data Science journey — Why and How](https://towardsdatascience.com/use-kaggle-to-start-and-guide-your-ml-data-science-journey-f09154baba35) 

[Make your Pandas apply functions faster using Parallel Processing](https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1)

## Week 2 - 15 hours :

**Theory** : 

More on gradient boosting. SGB. GB for classification and regression. GB for decision trees.

Neural networks. Tutorials from course below. One layer NN. Universal approximation theorem. Various activation functions. Multilayer NN. Optimization of NN parameters. GD, SGD.  Forward Propagation and Backpropagtion. Regularization.

**Practice** : 

First practice from week 4 - decision trees, bagging and random forest.

[Bagging and random forest](https://colab.research.google.com/drive/1UA6fSB1_edpchK3Y5yYv19KGtm6icw2y)

[Let's take a look at XGBoost](https://colab.research.google.com/drive/1k6vYhFmk8Zh029RcpcXv_6NKM3DaYiTP)

[Finished week 4. Gradient boosting practice.](https://colab.research.google.com/drive/1qVHT21XzXJ00iMi7ZCjocQcTNZkmss64)

3 tensorflow videos - [TensorFlow high-level APIs: Part 1 - loading data - YouTube](https://www.youtube.com/watch?v=oFFbKogYdfc)

https://www.coursera.org/learn/introduction-tensorflow/home/welcome - whole course

Finished [Pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

Checked notebook about decesion trees and random forests from there (it was pretty simple):

https://colab.research.google.com/drive/1HindO6SmgirUQOT3TOWj-hwJroUQ52Es

Made a notebook for this article:
[Introduction to Python Ensembles – Dataquest](https://www.dataquest.io/blog/introduction-to-ensembles/):

https://colab.research.google.com/drive/1OvgNn6dEQ7BSeC8NvKy4rFMtfTa1oLVU

**Other** :

[Wonderful article about ensemble methods](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)

## Week 3 (short one to start next one on Monday ) - 12 hours

**Theory** : 

Naive Bayes. Bayes' classifier with Maximum Likelihood Estimation. Bayes minimum risk decision rule.

KNN. Params and metrics in KNN. SVM, Kernel Trick; Mercer’s Theorem.

**Practice** : 

[Neural networks practice](https://colab.research.google.com/drive/1sjx1eiiVriHvpaS7trWov9EacTLDKYaz) - not translated!

[Event models of Naive Bayes](https://colab.research.google.com/drive/1hyriE27Q7Mkb5-VpfA_I4jiQon31ltIf)

[KNN vs RF](https://colab.research.google.com/drive/1CROYtKjQUCuIV1_iStJU-ghKSTyQS_aD)

**Other** : 

[neural-network-forward-propagation-and-backpropagtion](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76)

## Week 4 - 11 hours

**Theory** : 

Started Andrew NG - 7. Large Margin Classification. Optimization Objective. Large margin intuition. Mathematics Behind Large Margin Classification.

[SVM notebook](https://colab.research.google.com/drive/19gdFffFEzasKAoY1JmLnUtPhCq3D-yQ5) from the book

Kernels, more about math behind SVM.

Clustering with K-Means and EM. K-means and Mixtures of Gaussians


**Practice** : 

Finished practice from Andrew NG week 7.

[SVM](https://colab.research.google.com/drive/1BZZjaOwEjI9WH8M_u2ZTQUEummw1q3Hf)

**Other** : 

Translating older notebooks.

[Siraj Raval - Mathematics for Beginners video](https://www.youtube.com/watch?v=sb_FI7nt4yk)

A lot of articles about statistic.

https://medium.com/diogo-menezes-borges/introduction-to-statistics-for-data-science-16a188a400ca
https://towardsdatascience.com/statistics-is-the-grammar-of-data-science-part-1-c306cd02e4db
https://towardsdatascience.com/the-5-basic-statistics-concepts-data-scientists-need-to-know-2c96740377ae
https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1

## Week 5 - 14 hours

**Theory** : 

Practicing k-means and em for text clustering. More on K-means and EM theory. Agglomerative Hierarchical Clustering, dendograms. SDG and PCA. Density-based methods. DBSCAN. Silhoette analysis. 

Andrew NG, week 8. Data compression, Vizualization. PCA problem formulation, alghorithm.

**Practice** : 

Finished practices from Andrew NG and Yandex weeks:

Example of clustering tasks for a company - [Banners](https://colab.research.google.com/drive/16QMB94dp4EB4jEiQPaqW5jVuV6bwUN9-)

[SVM](https://colab.research.google.com/drive/1BZZjaOwEjI9WH8M_u2ZTQUEummw1q3Hf)

[K-means and PCA](https://colab.research.google.com/drive/1NN23kLE0Pe5XZqzX_HpLCGVSTPRT8dti)

2 notebooks from the Big Book to rule them all:

[Wonderful notebook about PCA](https://colab.research.google.com/drive/17GArLecx5R8GJto6zfDOwQiBMvRgrhLh)

[K-means once again](https://colab.research.google.com/drive/170KIxk9u6x6pY08NH9vaswnMc9bUBAWR)

**Other** : 

Translating older notebooks.

## Week 6 - 10 hours

**Theory** : 

A lot of it. Yandex, week 2. Finished Python Data Science Handbook.

Dimensionality reduction and feature selection. One-dimensional FS. Greedy FS methods. Model-based FS. Dimensionality reduction. PCA. Matrix decomposition. SGD and ALS. Probabilistic look on Matrix decomposition.

**Practice** : 

[In-Depth: Manifold Learning](https://colab.research.google.com/drive/1evSSu29GPUZGlFg1gKDJd4oCLM2f6hcc)

[Gaussian Mixture Models](https://colab.research.google.com/drive/12Tocpu3awHMjbCRT3ff7-4BUnKpWLBWN)
Akaike information criterion, Bayesian information criterion

[Kernel Density Estimation](https://colab.research.google.com/drive/1rn2wY_PSlw7bL0mHZhU4t5YjPn-tKJgM)

[Face recognition with SVM](https://colab.research.google.com/drive/1UOcnP2t-MCRcUAhh8mc2G9ZnCSsTqT6r)

[Yandex week practice task - PCA](https://colab.research.google.com/drive/1C1mzq6gbbnxso4fum4WxOEOV-8qrNkms)

**Other** : 

Watched some PyData videos, Read a lot - KDnuggets, medium.

[I don’t like notebooks.- Joel Grus (Allen Institute for Artificial Intelligence) - YouTube](https://www.youtube.com/watch?v=7jiPeIFXb6U)

[How to Build a Healthcare Startup](https://www.youtube.com/watch?v=b8xlCNzkX5w)

[Zachariah Miller: The necessity of pipelines in Natural Language Processing | PyData Miami 2019](https://www.youtube.com/watch?v=SG6jdlBx_vQ)

A couple of videos about Jupiter Lab.


## Week 7

### Day 1-2 - 4 hours (estimated time of listening ML talks)

Was on https://pycon.ru/2019/en/ conference.

### Day 3-4 - 2 hour

**Theory** : 

Anomaly Detection.  Unsupervised Anomaly Detection Algorithms. Parametric and non-parametric approaches. SVM for anomaly detection.

**Other** : 

[Top 10 Statistics Mistakes Made by Data Scientists – Towards Data Science](https://towardsdatascience.com/top-10-statistics-mistakes-made-by-data-scientists-2d58ccf12ab1)

[What 70% of Data Science Learners Do Wrong – Towards Data Science](https://towardsdatascience.com/what-70-of-data-science-learners-do-wrong-ac35326219e4)

[5 Useful Statistics Data Scientists Need to Know](https://www.kdnuggets.com/2019/06/statistics-data-scientists-know.html)

### Day 5 - 1.5 hour

**Theory** : 

Data vizualization. Multiple scaling. t-SNE. 

**Practice** : 

[Data vizualization](https://colab.research.google.com/drive/1SlznaxmNIHxLFNAdC9w9yQDijP8FcIes)

