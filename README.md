# Path to ML world

Hello world!
100 Days of Machine Learning Coding started inspired by Sirj Raval idea -> transformed into path to great ML junior.

My related knowledge stack — python, a basic level of SQL, basics of math analysis linear algebra, probability theory. All the stuff you would expect to learn in a tech university. I am halfway through the Andrew NG ML course and the second Yandex course in the ML and Data analytics specialization.

## First step - Finished

2 big learning courses. I selected classic Stanford ML and Yandex ML specialization. I think I will translate collab notebooks with exercises in English if anybody will have an interest in them.

## Second step - Currently running

2 Advanced courses, dive into Neural Networks

## Third step 

A couple of projects maybe ?

## Month 1 total: 59 hours

## Week 1 - 21 hours :

**Theory** : 

Linear model. MSE. Regression. Regularization (l1,l2). Logistic regression. Fisher's linear discriminant. Generalized linear models. Prediction of probabilities and estimation of parameters.

Practical advices on linear models. Feature scaling. Straightening spaces. Work with categorical features. Work with Unbalanced data. Multi-class classification.

Naive Bayes. Links: [1](https://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/),
[2](http://dataaspirant.com/2017/02/20/gaussian-naive-bayes-classifier-implementation-python/)

Tensorflow piplines e.t.c. from io19. 

[Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) - Really great brief on the theme.

[The numpy chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb) - all written in notebooks, you can test everything, when reading. 


Decision tree. Overfitting. Stopping criteria. Gini Index and Information Gain. 

[Half of the pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

Tree pruning. Random Forest. Ensemble methods: bagging, boosting and stacking.OOB.

**Practice** : 

[Naive 1 notebook](https://colab.research.google.com/drive/1UnSuv1SsrW_8-uvIjCOKoE9UGhE3gdFT),
[Naive 2 notebook](https://colab.research.google.com/drive/1DdPtNxKLTjgk1PE1rp9xTb4IHSgyp6rF)

[Sklearn grid search](https://colab.research.google.com/drive/1TugvgI8zGuGqFPi8ckWCR4AtJSIfB6Ab)

[Bike sharing demand. Sklearn case 1](https://colab.research.google.com/drive/1z1P9LKDxNjkcKSrdHeK5Q0c_h1lI8opv)

[Bike sharing demand. Sklearn case 2](https://colab.research.google.com/drive/1fA_ySZ75olMrHuV-gt2G4nsWPQi9vLcD)

[Data preprocessing and logistic regression for binary classification tasks](https://colab.research.google.com/drive/15gX4xDvG_8BkqSts3EMbkz4W5ZivndL-)

[Decision trees](https://colab.research.google.com/drive/1t35u2Vf88RpG-UVKW368FLQlkSPZN_DP)

[Random forest](https://colab.research.google.com/drive/1FcLkZPbIF-do9TrS-nOudwETaR-NS9ze)

**Other** :

PyData Ann Arbor: Caitlin Hudon - [Building Data Science](https://www.youtube.com/watch?v=WlBamzdiwic)

Digging into collab and this github repo to make it beautiful :)

[Getting Started with TensorFlow 2.0 (Google I/O'19)](https://www.youtube.com/watch?v=lEljKc9ZtU8) #io19

[TensorFlow Extended: Machine Learning Pipelines and Model Understanding (Google I/O'19)](https://www.youtube.com/watch?v=drYM04t57tU)

[Google Colab Free GPU Tutorial – Deep Learning Turkey – Medium](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) - Treasure article

[Use Kaggle to start (and guide) your ML/ Data Science journey — Why and How](https://towardsdatascience.com/use-kaggle-to-start-and-guide-your-ml-data-science-journey-f09154baba35) 

[Make your Pandas apply functions faster using Parallel Processing](https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1)

## Week 2 - 15 hours :

**Theory** : 

More on gradient boosting. SGB. GB for classification and regression. GB for decision trees.

Neural networks. Tutorials from course below. One layer NN. Universal approximation theorem. Various activation functions. Multilayer NN. Optimization of NN parameters. GD, SGD.  Forward Propagation and Backpropagtion. Regularization.

**Practice** : 

First practice from week 4 - decision trees, bagging and random forest.

[Bagging and random forest](https://colab.research.google.com/drive/1UA6fSB1_edpchK3Y5yYv19KGtm6icw2y)

[Let's take a look at XGBoost](https://colab.research.google.com/drive/1k6vYhFmk8Zh029RcpcXv_6NKM3DaYiTP)

[Finished week 4. Gradient boosting practice.](https://colab.research.google.com/drive/1qVHT21XzXJ00iMi7ZCjocQcTNZkmss64)

3 tensorflow videos - [TensorFlow high-level APIs: Part 1 - loading data - YouTube](https://www.youtube.com/watch?v=oFFbKogYdfc)

https://www.coursera.org/learn/introduction-tensorflow/home/welcome - whole course

Finished [Pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

Checked notebook about decesion trees and random forests from there (it was pretty simple):

https://colab.research.google.com/drive/1HindO6SmgirUQOT3TOWj-hwJroUQ52Es

Made a notebook for this article:
[Introduction to Python Ensembles – Dataquest](https://www.dataquest.io/blog/introduction-to-ensembles/):

https://colab.research.google.com/drive/1OvgNn6dEQ7BSeC8NvKy4rFMtfTa1oLVU

**Other** :

[Wonderful article about ensemble methods](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)

## Week 3 (short one to start next one on Monday ) - 12 hours

**Theory** : 

Naive Bayes. Bayes' classifier with Maximum Likelihood Estimation. Bayes minimum risk decision rule.

KNN. Params and metrics in KNN. SVM, Kernel Trick; Mercer’s Theorem.

**Practice** : 

[Neural networks practice](https://colab.research.google.com/drive/1sjx1eiiVriHvpaS7trWov9EacTLDKYaz) - not translated!

[Event models of Naive Bayes](https://colab.research.google.com/drive/1hyriE27Q7Mkb5-VpfA_I4jiQon31ltIf)

[KNN vs RF](https://colab.research.google.com/drive/1CROYtKjQUCuIV1_iStJU-ghKSTyQS_aD)

**Other** : 

[neural-network-forward-propagation-and-backpropagtion](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76)

## Week 4 - 11 hours

**Theory** : 

Started Andrew NG - 7. Large Margin Classification. Optimization Objective. Large margin intuition. Mathematics Behind Large Margin Classification.

[SVM notebook](https://colab.research.google.com/drive/19gdFffFEzasKAoY1JmLnUtPhCq3D-yQ5) from the book

Kernels, more about math behind SVM.

Clustering with K-Means and EM. K-means and Mixtures of Gaussians


**Practice** : 

Finished practice from Andrew NG week 7.

[SVM](https://colab.research.google.com/drive/1BZZjaOwEjI9WH8M_u2ZTQUEummw1q3Hf)

**Other** : 

Translating older notebooks.

[Siraj Raval - Mathematics for Beginners video](https://www.youtube.com/watch?v=sb_FI7nt4yk)

A lot of articles about statistic.

https://medium.com/diogo-menezes-borges/introduction-to-statistics-for-data-science-16a188a400ca
https://towardsdatascience.com/statistics-is-the-grammar-of-data-science-part-1-c306cd02e4db
https://towardsdatascience.com/the-5-basic-statistics-concepts-data-scientists-need-to-know-2c96740377ae
https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1

## Month 2 total: 45 hours

## Week 5 - 14 hours

**Theory** : 

Practicing k-means and em for text clustering. More on K-means and EM theory. Agglomerative Hierarchical Clustering, dendograms. SDG and PCA. Density-based methods. DBSCAN. Silhoette analysis. 

Andrew NG, week 8. Data compression, Vizualization. PCA problem formulation, alghorithm.

**Practice** : 

Finished practices from Andrew NG and Yandex weeks:

Example of clustering tasks for a company - [Banners](https://colab.research.google.com/drive/16QMB94dp4EB4jEiQPaqW5jVuV6bwUN9-)

[SVM](https://colab.research.google.com/drive/1BZZjaOwEjI9WH8M_u2ZTQUEummw1q3Hf)

[K-means and PCA](https://colab.research.google.com/drive/1NN23kLE0Pe5XZqzX_HpLCGVSTPRT8dti)

2 notebooks from the Big Book to rule them all:

[Wonderful notebook about PCA](https://colab.research.google.com/drive/17GArLecx5R8GJto6zfDOwQiBMvRgrhLh)

[K-means once again](https://colab.research.google.com/drive/170KIxk9u6x6pY08NH9vaswnMc9bUBAWR)

**Other** : 

Translating older notebooks.

## Week 6 - 10 hours

**Theory** : 

A lot of it. Yandex, week 2. Finished Python Data Science Handbook.

Dimensionality reduction and feature selection. One-dimensional FS. Greedy FS methods. Model-based FS. Dimensionality reduction. PCA. Matrix decomposition. SGD and ALS. Probabilistic look on Matrix decomposition.

**Practice** : 

[In-Depth: Manifold Learning](https://colab.research.google.com/drive/1evSSu29GPUZGlFg1gKDJd4oCLM2f6hcc)

[Gaussian Mixture Models](https://colab.research.google.com/drive/12Tocpu3awHMjbCRT3ff7-4BUnKpWLBWN)
Akaike information criterion, Bayesian information criterion

[Kernel Density Estimation](https://colab.research.google.com/drive/1rn2wY_PSlw7bL0mHZhU4t5YjPn-tKJgM)

[Face recognition with SVM](https://colab.research.google.com/drive/1UOcnP2t-MCRcUAhh8mc2G9ZnCSsTqT6r)

[Yandex week practice task - PCA](https://colab.research.google.com/drive/1C1mzq6gbbnxso4fum4WxOEOV-8qrNkms)

**Other** : 

Watched some PyData videos, Read a lot - KDnuggets, medium.

[I don’t like notebooks.- Joel Grus (Allen Institute for Artificial Intelligence) - YouTube](https://www.youtube.com/watch?v=7jiPeIFXb6U)

[How to Build a Healthcare Startup](https://www.youtube.com/watch?v=b8xlCNzkX5w)

[Zachariah Miller: The necessity of pipelines in Natural Language Processing | PyData Miami 2019](https://www.youtube.com/watch?v=SG6jdlBx_vQ)

A couple of videos about Jupiter Lab.


## Week 7 - 10 hours

**Theory** : 

Was on https://pycon.ru/2019/en/ conference.

Data vizualization. Multiple scaling. t-SNE. 

Anomaly Detection. Unsupervised Anomaly Detection Algorithms. Parametric and non-parametric approaches. SVM for anomaly detection.

Density Estimation. Building an Anomaly Detection System. Anomaly detection vs supervised learning.Choosing features.


**Practice** : 

[Data vizualization](https://colab.research.google.com/drive/1SlznaxmNIHxLFNAdC9w9yQDijP8FcIes)

[VisualizationPeerReview practice](https://colab.research.google.com/drive/1j_1MlcBv7eyOYUilzqEv9AZynz5uPb0K)


**Other** : 

[Top 10 Statistics Mistakes Made by Data Scientists – Towards Data Science](https://towardsdatascience.com/top-10-statistics-mistakes-made-by-data-scientists-2d58ccf12ab1)

[What 70% of Data Science Learners Do Wrong – Towards Data Science](https://towardsdatascience.com/what-70-of-data-science-learners-do-wrong-ac35326219e4)

[5 Useful Statistics Data Scientists Need to Know](https://www.kdnuggets.com/2019/06/statistics-data-scientists-know.html)


## Week 8 - 11 hours

**Theory** : 

Multivariate Gaussian Distribution. Anomaly Detection using the Multivariate Gaussian Distribution. Predictiing movie ratings - Problem Formulation. Content Based Recommendations. Collaborative Filtering Algorithm. 

Vectorization: Low Rank Matrix Factorization. Implementational Detail: Mean Normalization. Started last practice exercise.

Week 10 of Stanford's ML Gradient Descent with Large Datasets. Learning With Large Datasets. Stochastic Gradient Descent. Mini-Batch Gradient Descent. Stochastic Gradient Descent Convergence. Online Learning. Map Reduce and Data Parallelism.

#### Finished [Stanford ML Course](https://www.coursera.org/learn/machine-learning/home/info) 
Week 11 - Problem Description and Pipeline. Sliding Windows. Geting Lots of Data and Artificial Data. Ceiling Analysis. Photo OCR.

Topic modeling in texts. Topic modeling problem statement. Algorithm of topic modeling. Regularized EM-algorithm. Bayes LDA. Non-bayes LDA. Kullback-Leibler divergence. Regularization in topic modeling. Additive Regularization of Topic Models. 

**Practice** : 

[Last Andrew NG Course exercise - Anomaly Detection and Recommender Systems](https://colab.research.google.com/drive/1qgLaZMQNInt_24CCePtzYQoOAv_07P09)

**Other** : 

https://towardsdatascience.com/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66 - great short review of NLP methods

Some articles about google collab:
https://towardsdatascience.com/google-colab-59ad8510eb7e
https://medium.com/@burakteke/tutorial-on-using-google-colab-for-kaggle-competition-620393c22821

https://colab.research.google.com/notebooks/io.ipynb - great notebook about working with External data: Local Files, Drive, Sheets, and Cloud Storage in collab

## Month 3 total: 44 hours

## Week 9.1 (5 days) - 7 hours

**Theory** : 

Topic modeling. Basic assumptions of simple topic models. The task of topic modeling as a matrix decomposition problem. Basic topic modeling models and EM-algorithm. Bayes LDA. Non bayes LDA. ARTM. Model metrics of quality - perplexity, hold-out perplexity, coherence. 

**Practice** :

2 notebooks on LDA and BigARTM lib waiting to be trenslated in english :)

# Vacation break until the end of July
I decided to take a break from the challenge for the time I will be on vacation. Tho, I have plans to finalize all that I learned so far and translate all notebooks in English. And starting from August I am planning to continue my journey ^_^


# Aaaaaaaaaaand I am back :) 
## One week after vacation has already passed. Of course i havent learned anything during the trip across the Italy (there were some things to do). So the plan for now is to refresh everything i've learned so far and start the second part of the journey !

## Week 9.2 (5 days) - 7 hours
### Week 9 is devided by 3 weeks and the second part of it is tributed to just refresh everything that i've learned so far. So, i decided not to declare it as week 10, but as the second part of week 9. 

**Theory** : 

Repeated Andrew NG course.
Repeated Yandex course.

**Practice** : 

Repeated all notebooks above.

**Other**

The following KDnuggets Top 10 list highlights the most active and popular podcasts so far in 2019 that feature data science and machine learning conversations. https://www.kdnuggets.com/2019/07/best-podcasts-ai-analytics-data-science-machine-learning.html

https://www.kdnuggets.com/2019/08/what-data-science-learners-do-wrong.html

https://www.kdnuggets.com/2019/06/random-forest-vs-neural-network.html

## Week 10 - 7 hours

**Theory** : 

Sarted statistics course. Disributions. Quantile. Confidence interval. Confidence Interval for a Population Mean. Confidence interval for a proportion, 2 (in)dependant proportions. Bootstrap Confidence Intervals. Null Hypothesis. Type I and type II errors. p-value. Connection between hypothesis checking and confidence intervals. Pearson’s Correlation. 

**Practice** : 

[Confidence Interval for a Population Mean](https://colab.research.google.com/drive/1WzvykK2JYX4g3Kd5rMCrsA0vPJM7Zfwr)

[Confidence Interval for a proportion](https://colab.research.google.com/drive/1YiN5u-UkHk8a0LEPJlUOS8H1xDQkgrbz)

[Confidence interval for 2 proportions](https://colab.research.google.com/drive/1QGhPeEpSm2WJZNhWNXOT92O6hC-xm84I)

[Bootstrap Confidence Intervals](https://colab.research.google.com/drive/1LhhA-wlHdpwY4T6UZ1qrYmfxsj_TFTLc)

[Binominal test for a sample](https://colab.research.google.com/drive/1lSBcz8_gw4cEqQftkOIOLQfZkqW_duHn)

[Confidence Intervals for Pearson’s Correlation](https://colab.research.google.com/drive/16n0VICdZeLnrF5WPz-7UwxyT6KFT2LVA)

[Confidence interval practice](https://colab.research.google.com/drive/1_h8J4dEEFEhGYN6pxLjIRm2lJ5l-Cnvw)

**Other**

https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/confidence-interval/

https://cran.r-project.org/web/packages/dabestr/vignettes/bootstrap-confidence-intervals.html

https://medium.com/@Nithanaroy/3-ways-to-design-affective-classes-in-ml-classification-algorithms-57a302e5397b

https://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8

https://www.kdnuggets.com/2019/06/statistics-mistakes-data-scientists.html

https://towardsdatascience.com/3-strategies-to-guarantee-a-data-science-job-with-no-experience-68d85b345f21

https://towardsdatascience.com/data-scientists-the-five-graph-algorithms-that-you-should-know-30f454fa5513


## Week 11 - 12 hours

**Theory** : 

More work on statistics - same topics, some calculations on paper to better understand it. 

A/B testing. Conditions, Metrics. How to create groups e.t.c. Experiment design. A/A testing.  

Hypothesis testing. Parametric and Non-Parametric statistical tests.

Student's tests, p-value, T-Score vs. Z-Score e.t.c. One- and two-tailed tests. Shapiro–Wilk test. 

Wilcoxon signed rank test, permutation test, bootstap test. Mann-Whitney test. 

**Practice** : 

Installed jupyter lab. 

[Student's test 1](https://colab.research.google.com/drive/1y87zgFan234idjXPOW5cEUlrWpkPNr2Q)

[Student's test 2](https://colab.research.google.com/drive/1gKQhnz5nIafDvWvAq00jKwxSJJkMZRNU)

[Z-test for 2 proportions](https://colab.research.google.com/drive/1EYgTZj6TZqMglNuIiTVcxcDh0yC-Jk3V)

[Exercises of the chapter 2 (Z-test)](https://colab.research.google.com/drive/1lKHhzLND1yqJlryWRB121yNKEeWQMA_N)

[Non-parametric tests 1](https://colab.research.google.com/drive/1hXjePPo3AVSfX9DsEfODVJi0_4NFgQW5)

[Non-parametric tests 2](https://colab.research.google.com/drive/1uRGFBmPZCTnaK7XV0QLi9u8GLD8MVprI)

[Non-parametric tests 3](https://colab.research.google.com/drive/1jaKHeJvc95J9asZ0PxSZuWyddHnqm913)

[Non-parametric tests practice exercise](https://colab.research.google.com/drive/1ZhIZrLUg5P-AYoFw3PjX-kt7cI08cYQe)

**Other**

Started Practically Cheating Statistics Handbook

https://en.wikipedia.org/wiki/One-_and_two-tailed_tests

https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test

https://www.statisticshowto.datasciencecentral.com/parametric-and-non-parametric-data/


## Week 12 - 12 hours

**Theory** : 

Correlation. Pearson corr. Spearman Corr. Mathews corr. Cramer's V.

[Cool game - guess the correlation](http://guessthecorrelation.com/)

Correction for multiple hypothesis testing. Bonferroni Correction. Holm’s Step-Down Procedure. Benjamini-Hochberg Procedure.

**Practice** : 

[Pearson correlation](https://colab.research.google.com/drive/1lt0BaENsBWo-omOqvyIvye3pZc6I9YpB)

[Pearson/Spearman correlation](https://colab.research.google.com/drive/1pZ2gHF9AVVp5VQZtFZZEYWpfIBRXtQpQ)

[Correlation exercisre](https://colab.research.google.com/drive/11HDyRHNtpIzGTT29SE2-W7yRRtAh0QeF)

[Multiple Hypothesis testing](https://colab.research.google.com/drive/1x78quFVNotSJP1PE2sOsCYdza_WbBPp8)

[Multiple Hypothesis testing test](https://colab.research.google.com/drive/1MqTC4q8bOjEI6i2YcmcgkAXtSSYj6ZGR)

[Full linear regression hypothesis testing lecture notebook](https://colab.research.google.com/drive/1AbYWWB4PkM7UgvZutX7vGK-0q7i_cuHv)

[Full linear regression hypothesis testing exercise](https://colab.research.google.com/drive/1hdEActQeQeQe19LFr_bK1v27V2rhK5jh)

**Other**

Finished Practically Cheating Statistics Handbook

## Month 4 total: 60 hours

## Week 13 - 15 hours of hell

**Practice** : 

4 big practice tasks on statistics.
I understood that i need to spend next week to repeat statistic course.
Because:
a. It's very important.
b. Unlike math, linear algebra, probability theory e.t.c it is my first encounter with it, so i want to ensure that i got it right.
c. I need to translate notebooks ¯\_(ツ)_/¯

[Detection of statistically significant differences in gene expression levels of cancer patients](https://colab.research.google.com/drive/1M6HY-PBIjO1y8EeYHlrQVqceOd3aIhuk)

[Data Analysis in Credit Scoring Problem](https://colab.research.google.com/drive/1n8M9fYRT-i2Csl7WkO0-myvqX451jTI9)

[AB new browser feature test analytics](https://colab.research.google.com/drive/1xeRiAv4w4xRz_1JdGCVhsujIpL9S4RKk)

[Churn analysis in telecom company](https://colab.research.google.com/drive/11fUg-QOvhQ3oVYdSf87oQoFAq1p0I4G7)


**Other** : 

Reading "Hands on machine learning with scikit learn and tensorflow v2."

## Week 14 - 12h

**Theory** : 

Started repeating.

**Practice** : 

Rolled in Open Data Science ML course. Created separate repository for it.

**Other** : 

[Great toll for small datasets](https://pypi.org/project/pandas-profiling/#description)

[Wonderful artcle about real DS and ML work](https://towardsdatascience.com/data-science-is-boring-1d43473e353e)

[6 Different Ways to Compensate for Missing Values In a Dataset](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)

## Week 15 - 

## Week 16 - 19

I kinda forgot about this repo. So i almost finished a book called hands on machine learning with scikit learn and tensorflow v2.0. Enrolled and participating in Open Data Science coursу and also reading a lot.

## Month 5 total: 60 hours

## Week 17

## Week 18

## Week 19

## Week 20

## Month 6 total: 60 hours

## Week 21

## Week 22

## Week 23

## Week 24

## Month 7 total: 

## Week 25 - 15h

**Theory** : 

Neural networks. PyTorch framework. Convolutional networks. 

**Practice** : 

MNIST classifier with NN. DogCats classifier with NN and convolutional NN. NN on synthesized datasets.

**Other** : 

4 great articles to understand basics of NN:

[CNN in PyTorch](https://algorithmia.com/blog/convolutional-neural-nets-in-pytorch)

[CNN theory from Stanford's CS231](http://cs231n.github.io/convolutional-networks/)

[Basics of image classification with pytorch](https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864)

[Dogs and cats CNN](http://ernie55ernie.github.io/machine%20learning/2019/01/06/dogs-and-cats-using-pretrained-convolution-neural-network-for-feature-extraction-and-prediction-with-pytorch.html)


## Week 26 - 14h

**Theory** : 

Search ranking. Metrics of search ranking: Acc, precision, DCG. Ranking methods: Listwise approach. Types of recommendation systems: Colaborative filtering, content-based. Hybrid systems. Offline and online metrics of recommendation systems.

Refreshing ingormation about preprocessing of datasets and pipelines.
XGBoost and Data leakage. 

**Practice** : 

Building simple recommendation system based on most viewed, most bought items for user sessions in the e-commerce shop.
Enrolled in Kaggle Intermediate Machine Learning course (decided to pass all of useful kaggle courses).

Finished Kaggle Intermediate Machine Learning course.

Finished Kaggle vizualization course.

**Other** : 

Returned here, trying to rember and fill evrth, i've done for 3 monthes XD
