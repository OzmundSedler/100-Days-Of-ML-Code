# 100-Days-Of-ML-Code
Hello world!
100 Days of Machine Learning Coding as proposed by Siraj Raval

First step — 2 big learning courses. I selected classic Stanford ML and Yandex ML specialization. I think I will translate collab notebooks with exercises in English if anybody will have an interest in them.

#100DaysOfMLCode

## Day 1 - 4 hours :

**Theory** : 

Linear model. MSE. Regression. Regularization (l1,l2). Logistic regression. Fisher's linear discriminant. Generalized linear models. Prediction of probabilities and estimation of parameters.

Practical advices on linear models. Feature scaling. Straightening spaces. Work with categorical features. Work with Unbalanced data. Multi-class classification.

**Practice** : 

Naive Bayes. Links: [1](https://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/),
[2](http://dataaspirant.com/2017/02/20/gaussian-naive-bayes-classifier-implementation-python/)

[Naive 1 notebook](https://colab.research.google.com/drive/1UnSuv1SsrW_8-uvIjCOKoE9UGhE3gdFT),
[Naive 2 notebook](https://colab.research.google.com/drive/1DdPtNxKLTjgk1PE1rp9xTb4IHSgyp6rF)

**Other** :

PyData Ann Arbor: Caitlin Hudon - [Building Data Science](https://www.youtube.com/watch?v=WlBamzdiwic)

Digging into collab and this github repo to make it beautiful :)

## Day 2 - 2.5 hours :

**Theory** : 

Tensorflow piplines e.t.c. from io19. 

**Practice** : 

[Sklearn grid search](https://colab.research.google.com/drive/1TugvgI8zGuGqFPi8ckWCR4AtJSIfB6Ab)

[Bike sharing demand. Sklearn case 1](https://colab.research.google.com/drive/1z1P9LKDxNjkcKSrdHeK5Q0c_h1lI8opv)

[Bike sharing demand. Sklearn case 2](https://colab.research.google.com/drive/1fA_ySZ75olMrHuV-gt2G4nsWPQi9vLcD)

**Other** :

[Getting Started with TensorFlow 2.0 (Google I/O'19)](https://www.youtube.com/watch?v=lEljKc9ZtU8) #io19

## Day 3 - 1.5 hours :

**Practice** : 

Doing coding exercise of Yandex week 3. Suffering from old libs functions. Refactoring it to new versions.

**Other** :

[TensorFlow Extended: Machine Learning Pipelines and Model Understanding (Google I/O'19)](https://www.youtube.com/watch?v=drYM04t57tU)

## Day 4 - 4 hours :

**Practice** : 

Finished coding exercise of Yandex week 3.

[Data preprocessing and logistic regression for binary classification tasks](https://colab.research.google.com/drive/15gX4xDvG_8BkqSts3EMbkz4W5ZivndL-)

Dunno if anybody will need it, but i plan to continue translating exercises from this course, becouse they are actually really good (i'd say like Andrew NG practice x2). I opened comments, so feel free to help XD

**Other** :

A lot of good articles today:

[Google Colab Free GPU Tutorial – Deep Learning Turkey – Medium](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) - Treasure article

[Use Kaggle to start (and guide) your ML/ Data Science journey — Why and How](https://towardsdatascience.com/use-kaggle-to-start-and-guide-your-ml-data-science-journey-f09154baba35) 

## Day 5 - 2 hours :

**Theory** : 

[https://en.wikipedia.org/wiki/Receiver_operating_characteristic] - Really great brief on the theme.

[The numpy chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb) - all written in notebooks, you can test everything, when reading. 

**Other** :

Was working on article for kdnuggets and medium about this challenge :)
Planning next week. 


## Day 6 - 4 hours :

**Theory** : 

Decision tree. Overfitting. Stopping criteria. Gini Index and Information Gain. 

[Half of the pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

**Other** :

Published my first [article](
https://medium.com/@ozmundsedler/100-days-of-machine-learning-code-e5ff109434a0) on Medium !


Checked all notebooks with grammlary :) Hope someone will read them XD


## Day 7 - 3 hours :

**Theory** : 

Tree pruning. Random Forest. Ensemble methods: bagging, boosting and stacking.OOB.

**Practice** :

[Decision trees](https://colab.research.google.com/drive/1t35u2Vf88RpG-UVKW368FLQlkSPZN_DP),
[Random forest](https://colab.research.google.com/drive/1FcLkZPbIF-do9TrS-nOudwETaR-NS9ze)

**Other** :

[Make your Pandas apply functions faster using Parallel Processing](https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1)

## Day 8 - 1 hour :

**Practice** : 

First practice from week 4 - decision trees, bagging and random forest.

[Bagging and random forest](https://colab.research.google.com/drive/1UA6fSB1_edpchK3Y5yYv19KGtm6icw2y),

**Other** :

[Wonderful article about ensemble methods](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)

## Day 9 - 1.5 hour :

**Theory** : 

More on gradient boosting. SGB. GB for classification and regression. GB for decision trees.

**Practice** : 

[Let's take a look at XGBoost](https://colab.research.google.com/drive/1k6vYhFmk8Zh029RcpcXv_6NKM3DaYiTP)
