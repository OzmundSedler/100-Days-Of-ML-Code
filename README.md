# 100-Days-Of-ML-Code
Hello world!
100 Days of Machine Learning Coding as proposed by Siraj Raval

First step — 2 big learning courses. I selected classic Stanford ML and Yandex ML specialization. I think I will translate collab notebooks with exercises in English if anybody will have an interest in them.

My related knowledge stack — python, a basic level of SQL, basics of math analysis linear algebra, probability theory. All the stuff you would expect to learn in a tech university. I am halfway through the Andrew NG ML course and the second Yandex course in the ML and Data analytics specialization.

#100DaysOfMLCode

## Month 1 total: 59 hours

## Week 1 - 21 hours :

**Theory** : 

Linear model. MSE. Regression. Regularization (l1,l2). Logistic regression. Fisher's linear discriminant. Generalized linear models. Prediction of probabilities and estimation of parameters.

Practical advices on linear models. Feature scaling. Straightening spaces. Work with categorical features. Work with Unbalanced data. Multi-class classification.

Naive Bayes. Links: [1](https://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/),
[2](http://dataaspirant.com/2017/02/20/gaussian-naive-bayes-classifier-implementation-python/)

Tensorflow piplines e.t.c. from io19. 

[Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) - Really great brief on the theme.

[The numpy chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb) - all written in notebooks, you can test everything, when reading. 


Decision tree. Overfitting. Stopping criteria. Gini Index and Information Gain. 

[Half of the pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

Tree pruning. Random Forest. Ensemble methods: bagging, boosting and stacking.OOB.

**Practice** : 

[Naive 1 notebook](https://colab.research.google.com/drive/1UnSuv1SsrW_8-uvIjCOKoE9UGhE3gdFT),
[Naive 2 notebook](https://colab.research.google.com/drive/1DdPtNxKLTjgk1PE1rp9xTb4IHSgyp6rF)

[Sklearn grid search](https://colab.research.google.com/drive/1TugvgI8zGuGqFPi8ckWCR4AtJSIfB6Ab)

[Bike sharing demand. Sklearn case 1](https://colab.research.google.com/drive/1z1P9LKDxNjkcKSrdHeK5Q0c_h1lI8opv)

[Bike sharing demand. Sklearn case 2](https://colab.research.google.com/drive/1fA_ySZ75olMrHuV-gt2G4nsWPQi9vLcD)

[Data preprocessing and logistic regression for binary classification tasks](https://colab.research.google.com/drive/15gX4xDvG_8BkqSts3EMbkz4W5ZivndL-)

[Decision trees](https://colab.research.google.com/drive/1t35u2Vf88RpG-UVKW368FLQlkSPZN_DP)

[Random forest](https://colab.research.google.com/drive/1FcLkZPbIF-do9TrS-nOudwETaR-NS9ze)

**Other** :

PyData Ann Arbor: Caitlin Hudon - [Building Data Science](https://www.youtube.com/watch?v=WlBamzdiwic)

Digging into collab and this github repo to make it beautiful :)

[Getting Started with TensorFlow 2.0 (Google I/O'19)](https://www.youtube.com/watch?v=lEljKc9ZtU8) #io19

[TensorFlow Extended: Machine Learning Pipelines and Model Understanding (Google I/O'19)](https://www.youtube.com/watch?v=drYM04t57tU)

[Google Colab Free GPU Tutorial – Deep Learning Turkey – Medium](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) - Treasure article

[Use Kaggle to start (and guide) your ML/ Data Science journey — Why and How](https://towardsdatascience.com/use-kaggle-to-start-and-guide-your-ml-data-science-journey-f09154baba35) 

[Make your Pandas apply functions faster using Parallel Processing](https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1)

## Week 2 - 15 hours :

**Theory** : 

More on gradient boosting. SGB. GB for classification and regression. GB for decision trees.

Neural networks. Tutorials from course below. One layer NN. Universal approximation theorem. Various activation functions. Multilayer NN. Optimization of NN parameters. GD, SGD.  Forward Propagation and Backpropagtion. Regularization.

**Practice** : 

First practice from week 4 - decision trees, bagging and random forest.

[Bagging and random forest](https://colab.research.google.com/drive/1UA6fSB1_edpchK3Y5yYv19KGtm6icw2y)

[Let's take a look at XGBoost](https://colab.research.google.com/drive/1k6vYhFmk8Zh029RcpcXv_6NKM3DaYiTP)

[Finished week 4. Gradient boosting practice.](https://colab.research.google.com/drive/1qVHT21XzXJ00iMi7ZCjocQcTNZkmss64)

3 tensorflow videos - [TensorFlow high-level APIs: Part 1 - loading data - YouTube](https://www.youtube.com/watch?v=oFFbKogYdfc)

https://www.coursera.org/learn/introduction-tensorflow/home/welcome - whole course

Finished [Pandas chapter of "Python Data Science Handbook"](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)

Checked notebook about decesion trees and random forests from there (it was pretty simple):

https://colab.research.google.com/drive/1HindO6SmgirUQOT3TOWj-hwJroUQ52Es

Made a notebook for this article:
[Introduction to Python Ensembles – Dataquest](https://www.dataquest.io/blog/introduction-to-ensembles/):

https://colab.research.google.com/drive/1OvgNn6dEQ7BSeC8NvKy4rFMtfTa1oLVU

**Other** :

[Wonderful article about ensemble methods](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)

## Week 3 (short one to start next one on Monday ) - 12 hours

**Theory** : 

Naive Bayes. Bayes' classifier with Maximum Likelihood Estimation. Bayes minimum risk decision rule.

KNN. Params and metrics in KNN. SVM, Kernel Trick; Mercer’s Theorem.

**Practice** : 

[Neural networks practice](https://colab.research.google.com/drive/1sjx1eiiVriHvpaS7trWov9EacTLDKYaz) - not translated!

[Event models of Naive Bayes](https://colab.research.google.com/drive/1hyriE27Q7Mkb5-VpfA_I4jiQon31ltIf)

[KNN vs RF](https://colab.research.google.com/drive/1CROYtKjQUCuIV1_iStJU-ghKSTyQS_aD)

**Other** : 

[neural-network-forward-propagation-and-backpropagtion](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76)

## Week 4 - 11 hours

**Theory** : 

Started Andrew NG - 7. Large Margin Classification. Optimization Objective. Large margin intuition. Mathematics Behind Large Margin Classification.

[SVM notebook](https://colab.research.google.com/drive/19gdFffFEzasKAoY1JmLnUtPhCq3D-yQ5) from the book

Kernels, more about math behind SVM.

Clustering with K-Means and EM. K-means and Mixtures of Gaussians


**Practice** : 

Finished practice from Andrew NG week 7.

[SVM](https://colab.research.google.com/drive/1BZZjaOwEjI9WH8M_u2ZTQUEummw1q3Hf)

**Other** : 

Translating older notebooks.

[Siraj Raval - Mathematics for Beginners video](https://www.youtube.com/watch?v=sb_FI7nt4yk)

A lot of articles about statistic.

https://medium.com/diogo-menezes-borges/introduction-to-statistics-for-data-science-16a188a400ca
https://towardsdatascience.com/statistics-is-the-grammar-of-data-science-part-1-c306cd02e4db
https://towardsdatascience.com/the-5-basic-statistics-concepts-data-scientists-need-to-know-2c96740377ae
https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1

## Month 2 total: 45 hours

## Week 5 - 14 hours

**Theory** : 

Practicing k-means and em for text clustering. More on K-means and EM theory. Agglomerative Hierarchical Clustering, dendograms. SDG and PCA. Density-based methods. DBSCAN. Silhoette analysis. 

Andrew NG, week 8. Data compression, Vizualization. PCA problem formulation, alghorithm.

**Practice** : 

Finished practices from Andrew NG and Yandex weeks:

Example of clustering tasks for a company - [Banners](https://colab.research.google.com/drive/16QMB94dp4EB4jEiQPaqW5jVuV6bwUN9-)

[SVM](https://colab.research.google.com/drive/1BZZjaOwEjI9WH8M_u2ZTQUEummw1q3Hf)

[K-means and PCA](https://colab.research.google.com/drive/1NN23kLE0Pe5XZqzX_HpLCGVSTPRT8dti)

2 notebooks from the Big Book to rule them all:

[Wonderful notebook about PCA](https://colab.research.google.com/drive/17GArLecx5R8GJto6zfDOwQiBMvRgrhLh)

[K-means once again](https://colab.research.google.com/drive/170KIxk9u6x6pY08NH9vaswnMc9bUBAWR)

**Other** : 

Translating older notebooks.

## Week 6 - 10 hours

**Theory** : 

A lot of it. Yandex, week 2. Finished Python Data Science Handbook.

Dimensionality reduction and feature selection. One-dimensional FS. Greedy FS methods. Model-based FS. Dimensionality reduction. PCA. Matrix decomposition. SGD and ALS. Probabilistic look on Matrix decomposition.

**Practice** : 

[In-Depth: Manifold Learning](https://colab.research.google.com/drive/1evSSu29GPUZGlFg1gKDJd4oCLM2f6hcc)

[Gaussian Mixture Models](https://colab.research.google.com/drive/12Tocpu3awHMjbCRT3ff7-4BUnKpWLBWN)
Akaike information criterion, Bayesian information criterion

[Kernel Density Estimation](https://colab.research.google.com/drive/1rn2wY_PSlw7bL0mHZhU4t5YjPn-tKJgM)

[Face recognition with SVM](https://colab.research.google.com/drive/1UOcnP2t-MCRcUAhh8mc2G9ZnCSsTqT6r)

[Yandex week practice task - PCA](https://colab.research.google.com/drive/1C1mzq6gbbnxso4fum4WxOEOV-8qrNkms)

**Other** : 

Watched some PyData videos, Read a lot - KDnuggets, medium.

[I don’t like notebooks.- Joel Grus (Allen Institute for Artificial Intelligence) - YouTube](https://www.youtube.com/watch?v=7jiPeIFXb6U)

[How to Build a Healthcare Startup](https://www.youtube.com/watch?v=b8xlCNzkX5w)

[Zachariah Miller: The necessity of pipelines in Natural Language Processing | PyData Miami 2019](https://www.youtube.com/watch?v=SG6jdlBx_vQ)

A couple of videos about Jupiter Lab.


## Week 7 - 10 hours

**Theory** : 

Was on https://pycon.ru/2019/en/ conference.

Data vizualization. Multiple scaling. t-SNE. 

Anomaly Detection. Unsupervised Anomaly Detection Algorithms. Parametric and non-parametric approaches. SVM for anomaly detection.

Density Estimation. Building an Anomaly Detection System. Anomaly detection vs supervised learning.Choosing features.


**Practice** : 

[Data vizualization](https://colab.research.google.com/drive/1SlznaxmNIHxLFNAdC9w9yQDijP8FcIes)

[VisualizationPeerReview practice](https://colab.research.google.com/drive/1j_1MlcBv7eyOYUilzqEv9AZynz5uPb0K)


**Other** : 

[Top 10 Statistics Mistakes Made by Data Scientists – Towards Data Science](https://towardsdatascience.com/top-10-statistics-mistakes-made-by-data-scientists-2d58ccf12ab1)

[What 70% of Data Science Learners Do Wrong – Towards Data Science](https://towardsdatascience.com/what-70-of-data-science-learners-do-wrong-ac35326219e4)

[5 Useful Statistics Data Scientists Need to Know](https://www.kdnuggets.com/2019/06/statistics-data-scientists-know.html)


## Week 8 - 11 hours

**Theory** : 

Multivariate Gaussian Distribution. Anomaly Detection using the Multivariate Gaussian Distribution. Predictiing movie ratings - Problem Formulation. Content Based Recommendations. Collaborative Filtering Algorithm. 

Vectorization: Low Rank Matrix Factorization. Implementational Detail: Mean Normalization. Started last practice exercise.

Week 10 of Stanford's ML Gradient Descent with Large Datasets. Learning With Large Datasets. Stochastic Gradient Descent. Mini-Batch Gradient Descent. Stochastic Gradient Descent Convergence. Online Learning. Map Reduce and Data Parallelism.

#### Finished [Stanford ML Course](https://www.coursera.org/learn/machine-learning/home/info) 
Week 11 - Problem Description and Pipeline. Sliding Windows. Geting Lots of Data and Artificial Data. Ceiling Analysis. Photo OCR.

Topic modeling in texts. Topic modeling problem statement. Algorithm of topic modeling. Regularized EM-algorithm. Bayes LDA. Non-bayes LDA. Kullback-Leibler divergence. Regularization in topic modeling. Additive Regularization of Topic Models. 

**Practice** : 

[Last Andrew NG Course exercise - Anomaly Detection and Recommender Systems](https://colab.research.google.com/drive/1qgLaZMQNInt_24CCePtzYQoOAv_07P09)

**Other** : 

https://towardsdatascience.com/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66 - great short review of NLP methods

Some articles about google collab:
https://towardsdatascience.com/google-colab-59ad8510eb7e
https://medium.com/@burakteke/tutorial-on-using-google-colab-for-kaggle-competition-620393c22821

https://colab.research.google.com/notebooks/io.ipynb - great notebook about working with External data: Local Files, Drive, Sheets, and Cloud Storage in collab

## Week 9.1 (5 days)

**Theory** : 

Topic modeling. Basic assumptions of simple topic models. The task of topic modeling as a matrix decomposition problem. Basic topic modeling models and EM-algorithm. Bayes LDA. Non bayes LDA. ARTM. Model metrics of quality - perplexity, hold-out perplexity, coherence. 

**Practice** :

2 notebooks on LDA and BigARTM lib waiting to be trenslated in english :)

# Vacation break until the end of July
I decided to take a break from the challenge for the time I will be on vacation. Tho, I have plans to finalize all that I learned so far and translate all notebooks in English. And starting from August I am planning to continue my journey ^_^


# Aaaaaaaaaaand I am back :) 
## One week after vacation has already passed. Of course i havent learned anything during the trip across the Italy (there were some things to do). So the plan for now is to refresh everything i've learned so far and start the second part of the journey !

## Week 9.2 (5 days)
### Week 9 is devided by 3 weeks and the second part of it is tributed to just refresh everything that i've learned so far. So, i decided not to declare it as week 10, but as the second part of week 9. 

**Theory** : 

Repeated Andrew NG course.
Repeated Yandex course.

**Practice** : 

Repeated all notebooks above.

**Other**

The following KDnuggets Top 10 list highlights the most active and popular podcasts so far in 2019 that feature data science and machine learning conversations. https://www.kdnuggets.com/2019/07/best-podcasts-ai-analytics-data-science-machine-learning.html

https://www.kdnuggets.com/2019/08/what-data-science-learners-do-wrong.html

https://www.kdnuggets.com/2019/06/random-forest-vs-neural-network.html

## Week 10 - 7 hours

**Theory** : 

Sarted statistics course. Disributions. Quantile. Confidence interval. Confidence Interval for a Population Mean. Confidence interval for a proportion, 2 (in)dependant proportions. Bootstrap Confidence Intervals. Null Hypothesis. Type I and type II errors. p-value. Connection between hypothesis checking and confidence intervals. Pearson’s Correlation. 

**Practice** : 

[Confidence Interval for a Population Mean](https://colab.research.google.com/drive/1WzvykK2JYX4g3Kd5rMCrsA0vPJM7Zfwr)

[Confidence Interval for a proportion](https://colab.research.google.com/drive/1YiN5u-UkHk8a0LEPJlUOS8H1xDQkgrbz)

[Confidence interval for 2 proportions](https://colab.research.google.com/drive/1QGhPeEpSm2WJZNhWNXOT92O6hC-xm84I)

[Bootstrap Confidence Intervals](https://colab.research.google.com/drive/1LhhA-wlHdpwY4T6UZ1qrYmfxsj_TFTLc)

[Binominal test for a sample](https://colab.research.google.com/drive/1lSBcz8_gw4cEqQftkOIOLQfZkqW_duHn)

[Confidence Intervals for Pearson’s Correlation](https://colab.research.google.com/drive/16n0VICdZeLnrF5WPz-7UwxyT6KFT2LVA)

[Confidence interval practice](https://colab.research.google.com/drive/1_h8J4dEEFEhGYN6pxLjIRm2lJ5l-Cnvw)

**Other**

https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/confidence-interval/
https://cran.r-project.org/web/packages/dabestr/vignettes/bootstrap-confidence-intervals.html

https://medium.com/@Nithanaroy/3-ways-to-design-affective-classes-in-ml-classification-algorithms-57a302e5397b
https://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8
https://www.kdnuggets.com/2019/06/statistics-mistakes-data-scientists.html
https://towardsdatascience.com/3-strategies-to-guarantee-a-data-science-job-with-no-experience-68d85b345f21
https://towardsdatascience.com/data-scientists-the-five-graph-algorithms-that-you-should-know-30f454fa5513


